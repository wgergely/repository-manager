# Test Health Audit — 2026-02-22

**Auditor:** Claude Opus 4.6 (4 parallel agents, 128 files inspected)
**Scope:** All 11 crates + workspace integration tests
**Focus:** Test taxonomy accuracy, fixture quality, format validation coverage, golden file integrity

---

## Executive Summary

The test suite has **1,310 `#[test]` functions across 128 files** and passes CI. But it
creates a **false sense of confidence**: no test validates that generated config files
actually work with the tools they claim to support. The tests prove the code can write
files — not that those files are correct.

Five structural problems require remediation:

| # | Problem | Severity | Phase |
|---|---------|----------|-------|
| TH-1 | Mislabeled test categories ("integration" tests that are unit tests) | MEDIUM | [Phase 1](../plans/2026-02-22-test-health-phase1-taxonomy.md) |
| TH-2 | Fixture duplication (4+ copies of `setup_git_repo()`) | MEDIUM | [Phase 2](../plans/2026-02-22-test-health-phase2-fixtures.md) |
| TH-3 | Zero format validation against tool schemas | CRITICAL | [Phase 3](../plans/2026-02-22-test-health-phase3-format-validation.md) |
| TH-4 | Self-referential golden files (generated by code under test) | HIGH | [Phase 4](../plans/2026-02-22-test-health-phase4-golden-files.md) |
| TH-5 | Tautological tests (~40% of tool tests only verify `fs::write` works) | HIGH | [Phase 5](../plans/2026-02-22-test-health-phase5-tautological-tests.md) |

---

## Test Inventory

### By Crate

| Crate | Unit (src/) | Integration (tests/) | Total | Notes |
|-------|-------------|---------------------|-------|-------|
| repo-tools | 247 | 68 | 315 | Highest count, lowest signal |
| repo-cli | 189 | 71 | 260 | CLI binary tests are legitimate |
| repo-core | 127 | 107 | 234 | Good coverage |
| repo-content | 103 | 45 | 148 | **Mislabeled** (TH-1) |
| repo-blocks | 91 | 0 | 91 | Unit-only, appropriate |
| repo-fs | 16 | 65 | 81 | Real I/O tests, strong |
| repo-meta | 58 | 6 | 64 | Good |
| repo-git | 18 | 34 | 52 | Real git calls, strong |
| repo-extensions | 45 | 0 | 45 | Unit-only, appropriate |
| repo-mcp | 35 | 0 | 35 | **Integration tests are empty** |
| repo-presets | 23 | 2 | 25 | Low coverage |
| Workspace | — | 60 | 60 | `mission_tests.rs` is best file in repo |
| **Total** | **952** | **358** | **1,310** | |

### By Category (Actual, Not Labeled)

| Actual Category | Count | Files | Description |
|-----------------|-------|-------|-------------|
| True unit tests | ~750 | 80+ | In-memory, no I/O, single function |
| Component tests (with tempdir) | ~350 | 30+ | Create temp filesystem, test multiple functions |
| CLI binary tests | ~70 | 3 | Invoke real `repo` binary via assert_cmd |
| Mission/scenario tests | ~60 | 1 | `mission_tests.rs` — builder pattern, multi-step |
| Empty/placeholder tests | ~2 | 2 | `repo-mcp/tests/` — 0 `#[test]` functions |
| True integration tests | **0** | 0 | No test ever invokes a real external tool |

---

## Finding TH-1: Mislabeled Test Categories

### Problem

Three files labeled "integration" contain zero integration behavior:

**`crates/repo-content/tests/integration_tests.rs`** (10 tests)
- Labeled: "Integration tests" / "end-to-end behavior"
- Actual: Calls `Document::parse_as()`, `insert_block()`, `diff()` — pure in-memory library usage
- No filesystem, no external process, no cross-crate boundary

**`crates/repo-content/tests/diff_integration_tests.rs`** (8 tests)
- Labeled: "Integration tests for semantic diff"
- Actual: 9 calls to `Document::parse()` and `doc1.diff(&doc2)` — pure function tests

**`crates/repo-tools/tests/integration_tests.rs`** (28 tests)
- Labeled: "integration tests"
- Actual: Creates TempDir, calls `integration.sync()`, reads file back — component tests

### Impact
Developers searching for integration test gaps will see these files and conclude the gap
is filled. The label actively hides the absence of real integration testing.

### Evidence
- `repo-content/tests/integration_tests.rs:1` — `//! Integration tests for repo-content crate`
- `repo-content/tests/diff_integration_tests.rs:1` — `//! Integration tests for semantic diff`
- Every test in these files operates entirely within a single crate's API

---

## Finding TH-2: Fixture Duplication

### Problem

`setup_git_repo()` and variants are defined **4+ times** with inconsistent implementations:

| Location | Implementation | Realism |
|----------|---------------|---------|
| `repo-core/src/backend/standard.rs:170-176` | Creates `.git/HEAD` + refs manually | Fake |
| `repo-core/tests/mode_tests.rs:57-71` | Creates `.git/HEAD` + refs manually | Fake |
| `repo-core/tests/sync_tests.rs:14-20` | Creates `.git` directory only | Fake |
| `repo-cli/src/commands/branch.rs:172-183` | `Command::new("git").arg("init")` | Real |
| `repo-git/tests/container_tests.rs:9-72` | Full bare repo + worktree + config | Real |
| `mission_tests.rs:42-123` | `TestRepo` builder with `git2::Repository::init()` | Real (best) |

### Impact
- Fake fixtures (`fs::create_dir(".git")`) can't test operations that need real git state
- Mixing fake and real git repos means test results depend on which fixture was used
- Changes to git setup logic must be updated in 4+ places

### Best Practice Already Exists
`mission_tests.rs` has a `TestRepo` builder (lines 31-123) that is well-designed but
**not shared** — it only exists in that one file.

### Evidence
- Only 1 shared test module exists: `crates/repo-presets/tests/common/mod.rs` (13 lines)
- `tempdir()` vs `TempDir::new()` used inconsistently (both valid, but signals no convention)

---

## Finding TH-3: Zero Format Validation Against Tool Schemas

### Problem

The repository generates config files for 13 tools. **No test validates that the
generated output actually works with the target tool.** Every tool test follows this
pattern:

```rust
let rules = vec![Rule { id: "X", content: "Y" }];
integration.sync(&context, &rules).unwrap();
let content = fs::read_to_string(file).unwrap();
assert!(content.contains("Y"));  // Only proves fs::write works
```

### Tool-by-Tool Analysis

| Tool | Config File | Format | What's Tested | What's Missing |
|------|------------|--------|---------------|----------------|
| VSCode | `.vscode/settings.json` | JSON | JSON parses back ✓ | No schema validation |
| Cursor | `.cursorrules` | Markdown | Contains rule text ✓ | No block marker validation |
| Claude | `CLAUDE.md` | Markdown | Contains rule text ✓ | No structure validation |
| Copilot | `.github/copilot-instructions.md` | Markdown | Contains text ✓ | No format validation |
| Windsurf | `.windsurfrules` | Markdown | Contains text ✓ | No format validation |
| Gemini | `GEMINI.md` | Markdown | Contains text ✓ | No structure validation |
| Aider | `.aider.conf.yml` | YAML | Contains text ✓ | No YAML parse-back |
| Antigravity | `.agent/rules/*.md` | Markdown | File exists ✓ | No naming convention check |
| JetBrains | config files | XML/JSON | Minimal ✓ | No IDE schema validation |
| Zed | config files | JSON | Minimal ✓ | No Zed settings schema |
| Cline | `.clinerules` | Markdown | Contains text ✓ | No format validation |
| Roo | `.roorules` | Markdown | Contains text ✓ | No format validation |
| Amazon Q | config files | Various | Contains text ✓ | No format validation |

### Impact
If a code change introduces malformed YAML in the aider config or breaks JSON structure
in VSCode settings, no test will catch it.

### Evidence
- `repo-tools/tests/integration_tests.rs` — 28 tests, all use `assert!(content.contains(...))`
- `mission_tests.rs` `consumer_verification` module — checks format validity but not
  schema compliance
- Zero occurrences of YAML parse-back in aider tests
- Zero occurrences of JSON Schema validation in VSCode tests

---

## Finding TH-4: Self-Referential Golden Files

### Problem

`crates/repo-core/tests/fixture_tests.rs` compares generated output against golden files
in `test-fixtures/expected/`. But the golden files were **generated by the same code
being tested**. If the code has a bug that produces malformed output, the golden file
also encodes the bug, and the test passes.

### Evidence

Files at risk:
- `test-fixtures/expected/cursor/.cursorrules`
- `test-fixtures/expected/claude/CLAUDE.md`
- `test-fixtures/expected/aider/.aider.conf.yml`

The `fixture_tests.rs:107-137` test `test_golden_file_cursor_output_matches_expected()`
generates output with the same `ToolSyncer` code and compares against the golden file.
The golden file was initially created by running the same code.

### Impact
Golden file tests that encode their own output are tautological — they can never
detect format regressions because the "expected" output has the same regression.

---

## Finding TH-5: Tautological Tests

### Problem

Approximately 40% of tool-layer tests are tautological: they verify that writing a
string to a file produces that string when read back.

### Pattern

```rust
// This tests that fs::write(path, content) followed by fs::read_to_string(path)
// returns content. It does NOT test tool integration.
let rules = vec![Rule { id: "test-rule", content: "Test content" }];
integration.sync(&context, &rules).unwrap();
let content = fs::read_to_string(path).unwrap();
assert!(content.contains("Test content"));
```

### Count

Approximately 120 tests across `repo-tools/` follow this pattern.

### What Would Be Non-Tautological

```rust
// Parse the output as the target tool would
let content = fs::read_to_string(path).unwrap();
let yaml: serde_yaml::Value = serde_yaml::from_str(&content).unwrap(); // Proves valid YAML
assert!(yaml.get("model").is_some()); // Proves expected key exists
```

---

## Bright Spots (What Works Well)

| Area | Why It's Good |
|------|--------------|
| `tests/integration/src/mission_tests.rs` | Builder pattern, multi-step scenarios, drift detection, consumer format checks |
| `crates/repo-git/tests/` | Real `git` binary invocations — legitimate integration |
| `crates/repo-fs/tests/` | Concurrency, atomic writes, permissions, path traversal — real OS testing |
| `crates/repo-cli/tests/integration_tests.rs` | Real binary invocation via `assert_cmd` |
| `crates/repo-content/` | Thorough unit testing of parsing, editing, diffing |
| Anti-pattern checklist in `_index.md` | Good testing mandate — the rules exist, just aren't enforced yet |

---

## Recommended Remediation Order

| Phase | Domain | Self-Contained? | Dependency |
|-------|--------|-----------------|------------|
| 1 | [Test Taxonomy & Renaming](../plans/2026-02-22-test-health-phase1-taxonomy.md) | Yes | None |
| 2 | [Shared Test Fixture Library](../plans/2026-02-22-test-health-phase2-fixtures.md) | Yes | None |
| 3 | [Format Validation Tests](../plans/2026-02-22-test-health-phase3-format-validation.md) | Yes | Phase 2 (uses shared fixtures) |
| 4 | [Golden File Rehabilitation](../plans/2026-02-22-test-health-phase4-golden-files.md) | Yes | Phase 3 (uses format validators) |
| 5 | [Tautological Test Elimination](../plans/2026-02-22-test-health-phase5-tautological-tests.md) | Yes | Phase 3 (replaces with format checks) |

Phases 1 and 2 can run in parallel. Phases 3-5 are sequential.

---

## Cross-References

- **Source audit chain**: [2026-02-22 Deep Implementation Audit](2026-02-22-deep-implementation-audit.md)
- **Prior test audit**: [P3-test-hygiene.md](../tasks/P3-test-hygiene.md) — findings there remain valid; this audit is additive
- **Roadmap**: [2026-02-17 Roadmap](../plans/2026-02-17-roadmap.md) — Phase 0.4 (test core value prop)
- **ADR decisions**: [ADR-001](../adr/02-decisions/ADR-001-extension-system-architecture.md), [ADR-004](../adr/02-decisions/ADR-004-mcp-server-integration.md)
- **Research**: [Testing practices](../research/) — tool-specific format expectations
- **Testing framework docs**: [Testing README](../testing/README.md), [Gap tracking](../testing/GAP_TRACKING.md)

---

*Audit completed: 2026-02-22*
*Methodology: 4 parallel exploration agents covering test locations, naming conventions, fixture patterns, and mocking/patching*
